{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"../population_shifts/ensemble_models\")\n",
    "# ! ls \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_exp.helpers import gen_samples, setup_logging, shift_features, psi\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_copy = lambda x: x.copy()\n",
    "\n",
    "data_dev_raw = gen_samples(n_samples = 100000, n_exp=3, n_unif=3, n_normal = 3)\n",
    "data_dev_raw.columns = [f\"f_{ix}\" for ix in range(data_dev_raw.shape[1])]\n",
    "\n",
    "data_dev = (\n",
    "    data_dev_raw\n",
    "    .pipe(make_copy)\n",
    "    .pipe(shift_features, cols_to_shift = data_dev_raw.columns, min_max_scaler_range=(1,100))\n",
    ")\n",
    "\n",
    "\n",
    "data_samp_raw = gen_samples(n_samples = 100000, n_exp=3, n_unif=3, n_normal = 3)\n",
    "data_samp_raw.columns = [f\"f_{ix}\" for ix in range(data_samp_raw.shape[1])]\n",
    "\n",
    "data_samp = (\n",
    "    data_samp_raw\n",
    "    .pipe(make_copy)\n",
    "    .pipe(shift_features, cols_to_shift = data_dev_raw.columns, min_max_scaler_range=(1,100))\n",
    ")\n",
    "\n",
    "data_valid_raw = gen_samples(n_samples = 10000, n_exp=3, n_unif=3, n_normal = 3)\n",
    "data_valid_raw.columns = [f\"f_{ix}\" for ix in range(data_valid_raw.shape[1])]\n",
    "\n",
    "data_valid = (\n",
    "    data_valid_raw\n",
    "    .pipe(make_copy)\n",
    "    .pipe(shift_features, cols_to_shift = data_dev_raw.columns, min_max_scaler_range=(1,100))\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [col for col in data_dev_raw.columns if col.startswith(\"f_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-attraction",
   "metadata": {},
   "source": [
    "### Generate the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from targen.data import target\n",
    "\n",
    "\n",
    "def get_samples(data_in, feature_list, n_samples = 100):\n",
    "    interaction_terms_1 = \"\".join([\n",
    "        \"0.5*f_0*f_2\",\n",
    "        \"-0.2*f_3*f_4\",\n",
    "        \"+0.27*f_7*f_1\",\n",
    "        \"+0.8*f_4/f_6\",\n",
    "        \"-0.8*f_1/f_0\",\n",
    "        \"+0.5*f_4/f_1\",\n",
    "    ])\n",
    "\n",
    "#     interaction_terms_2 = \"\".join([\n",
    "#         \"-1.5*f_2*(f_2-f_1)\",\n",
    "#         \"+0.09*f_2*f_4\",\n",
    "#         \"+0.6*f_7*f_1\",\n",
    "#         \"+0.8*f_4/(f_2+f_0+f_6)\",\n",
    "#         \"+0.8*f_1/(f_0-0.5*f_2+0.2)\", # the shift shoul reduce here the effect  \n",
    "#         \"+0.6*f_4/(f_1+0.2*f_7)\",\n",
    "#     ])\n",
    "    \n",
    "    interaction_terms_2 = \"\".join([\n",
    "        \"-1.5*f_2*(f_2-f_1)\",\n",
    "        \"+0.09*f_2*f_4\",\n",
    "        \"+0.6*f_7*f_1\",\n",
    "        \"+0.8*f_4/(f_2+f_0)\",\n",
    "        \"+0.8*f_1/(f_2)\", # the shift shoul reduce here the effect  \n",
    "        \"+0.6*f_4/(f_1+0.2*f_7)\",\n",
    "    ])\n",
    "\n",
    "    expressions_1 = {\n",
    "        'linear': '-12.5*f_0 + 2*f_4 -3.2*f_8',\n",
    "        'non_linear': '0.7*f_2**1.5 - 0.2*sin(f_7)- 0.9*log(f_8)',\n",
    "        'interaction': interaction_terms_1,\n",
    "        'uniform_noise': {\n",
    "            'weight':9.7\n",
    "        },\n",
    "        'gaussian_noise': {\n",
    "            'weight':1.4,\n",
    "            'mu_gaus': -1.5\n",
    "        }\n",
    "    }\n",
    "\n",
    "#     expressions_2 = {\n",
    "#         'linear': '-12.7*f_0 + 2.1*f_4 -3.2*f_6',\n",
    "#         'non_linear': '0.2*f_2**1.4 - 0.2*sin(f_7)- 0.9*log(f_6)',\n",
    "#         'interaction': interaction_terms_2,\n",
    "#         'uniform_noise': {\n",
    "#             'weight':3.8\n",
    "#         },\n",
    "#         'gaussian_noise': {\n",
    "#             'weight':10.4,\n",
    "#             'mu_gaus': 5.5\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "    expressions_2 = {\n",
    "        'linear': '-12.7*f_0 + 2.1*f_4 -3.2*f_6',\n",
    "        'non_linear': '0.2*f_2**1.4 - 0.2*sin(f_7)- 0.9*log(f_6)',\n",
    "        'interaction': interaction_terms_2,\n",
    "        'uniform_noise': {\n",
    "            'weight':1.8\n",
    "        },\n",
    "        'gaussian_noise': {\n",
    "            'weight':5.4,\n",
    "            'mu_gaus': 5.5\n",
    "        }\n",
    "    }\n",
    "\n",
    "    data_y_1 = target.get_target_and_contributions(data_in, expressions=expressions_1, imbalance = 0.4, \n",
    "                                                 drop_features=False)\n",
    "    data_y_2 = target.get_target_and_contributions(data_in, expressions=expressions_2, imbalance = 0.3, \n",
    "                                                 drop_features=False)\n",
    "\n",
    "    X = data_y_1[feature_list]\n",
    "    y1 = data_y_1['y']\n",
    "    y2 = data_y_2['y']\n",
    "    \n",
    "    sample_ix = np.random.randint(0,high = n_samples,size = y1.shape[0])\n",
    "    \n",
    "    print(f\"Total samples: {n_samples}, expected data per sample {y1.shape[0]/n_samples}\")\n",
    "    \n",
    "    return X, y1, y2, pd.Series(sample_ix, index = X.index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, y1_dev, y2_dev, dummy = get_samples(data_dev,features, n_samples = 1000)\n",
    "X_samp, y1_samp, y2_samp, samples = get_samples(data_samp,features, n_samples = 100)\n",
    "X_valid, y1_valid, y2_valid, dummy = get_samples(data_valid,features, n_samples = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_samp.groupby(samples).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-austria",
   "metadata": {},
   "source": [
    "# Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_1, y_test_1 = train_test_split(X_dev,y1_dev, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_2 = y2_dev.loc[X_train.index]\n",
    "y_test_2= y2_dev.loc[X_test.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-university",
   "metadata": {},
   "source": [
    "# Train model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "min_frac =int(0.1*X_train.shape[0])\n",
    "print(min_frac)\n",
    "\n",
    "xgb_model_1 = xgb.XGBClassifier(\n",
    "    max_depth = 2,\n",
    "    reg_lambda = 0,\n",
    "    num_leaves=4,\n",
    "    n_estimators=105,\n",
    "    min_child_samples=min_frac\n",
    ")\n",
    "\n",
    "eval_set_1 = [(X_train,y_train_1),(X_test, y_test_1)]\n",
    "\n",
    "xgb_model_1.fit(\n",
    "    X_train,\n",
    "    y_train_1,\n",
    "    eval_metric=[\"auc\",\"logloss\"], \n",
    "#     eval_names = ['validation_0','validation_1'],\n",
    "    eval_set=eval_set_1, \n",
    "    verbose=40, # after how many training sets you want the printout on the console\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(18,8))\n",
    "\n",
    "\n",
    "ax[0].plot(xgb_model_1.evals_result_['validation_0']['logloss'], label = \"training target\")\n",
    "ax[0].plot(xgb_model_1.evals_result_['validation_1']['logloss'], label = \"test target\")\n",
    "\n",
    "ax[1].plot(xgb_model_1.evals_result_['validation_0']['auc'])\n",
    "ax[1].plot(xgb_model_1.evals_result_['validation_1']['auc'])\n",
    "ax[0].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-thumb",
   "metadata": {},
   "source": [
    "# Train model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "min_frac =int(0.1*X_train.shape[0])\n",
    "print(min_frac)\n",
    "\n",
    "xgb_model_2 = xgb.XGBClassifier(\n",
    "    max_depth = 2,\n",
    "    reg_lambda = 0,\n",
    "    num_leaves=4,\n",
    "    n_estimators=105,\n",
    "    min_child_samples=min_frac\n",
    ")\n",
    "\n",
    "eval_set_2 = [(X_train,y_train_2),(X_test, y_test_2)]\n",
    "\n",
    "xgb_model_2.fit(\n",
    "    X_train,\n",
    "    y_train_2,\n",
    "    eval_metric=[\"auc\",\"logloss\"], \n",
    "#     eval_names = ['validation_0','validation_1'],\n",
    "    eval_set=eval_set_2, \n",
    "    verbose=40, # after how many training sets you want the printout on the console\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(18,8))\n",
    "\n",
    "\n",
    "ax[0].plot(xgb_model_2.evals_result_['validation_0']['logloss'], label = \"training target\")\n",
    "ax[0].plot(xgb_model_2.evals_result_['validation_1']['logloss'], label = \"test target\")\n",
    "\n",
    "ax[1].plot(xgb_model_2.evals_result_['validation_0']['auc'])\n",
    "ax[1].plot(xgb_model_2.evals_result_['validation_1']['auc'])\n",
    "ax[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tree_1 = 100\n",
    "n_tree_2 = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-guitar",
   "metadata": {},
   "source": [
    "# Now let's work on the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_samp, y1_samp, y2_samp, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ixs = np.sort(samples.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transferboost.models import XGBTransferLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tboost = XGBTransferLearner(xgb_model_1, verbosity=0, base_score=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "probas_model = list()\n",
    "probas_tboost = list()\n",
    "\n",
    "for i in tqdm.tqdm(sample_ixs):\n",
    "#     if i >0: break\n",
    "    \n",
    "    X_train_i = X_samp[samples==i]\n",
    "    y2_samp_i = y2_samp[samples==i]\n",
    "\n",
    "    xgb_i = xgb.XGBClassifier(\n",
    "        max_depth = 2,\n",
    "        reg_lambda = 0,\n",
    "        num_leaves=4,\n",
    "        n_estimators=n_tree_1,\n",
    "        min_child_samples=min_frac,\n",
    "        verbosity = 0\n",
    "    )\n",
    "    \n",
    "    xgb_i.fit(X_train_i,y2_samp_i, verbose=False)\n",
    "    \n",
    "    probas_i = xgb_i.predict_proba(X_valid)[:,1]\n",
    "    \n",
    "    xgb_tboost.fit(X_train_i,y2_samp_i)\n",
    "    \n",
    "#     t_full_probas_i = xgb_tboost.predict_proba(X_valid)[:,1]\n",
    "    t_opti_probas_i = xgb_tboost.predict_proba(X_valid, tree_index = n_tree_1)[:,1]\n",
    "    \n",
    "    probas_model.append(probas_i)\n",
    "    probas_tboost.append(t_opti_probas_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tree_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_model_df = pd.DataFrame(probas_model).T\n",
    "probas_model_df.columns = [f'sample_{ix}' for ix in probas_model_df.columns ]\n",
    "\n",
    "probas_tboost_df = pd.DataFrame(probas_tboost).T\n",
    "probas_tboost_df.columns = [f'sample_{ix}' for ix in probas_tboost_df.columns ]\n",
    "\n",
    "real_probas = pd.Series(xgb_model_2.predict_proba(X_valid)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 20\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(18,8))\n",
    "\n",
    "real_probas_sorted = real_probas.sort_values()\n",
    "\n",
    "\n",
    "tboost_mean = probas_tboost_df.loc[real_probas_sorted.index].mean(axis=1)\n",
    "tboost_std = probas_tboost_df.loc[real_probas_sorted.index].std(axis=1)\n",
    "\n",
    "model_mean = probas_model_df.loc[real_probas_sorted.index].mean(axis=1)\n",
    "model_std = probas_model_df.loc[real_probas_sorted.index].std(axis=1)\n",
    "\n",
    "ax[0].plot(model_mean.values[::step], color = 'blue', label = \"refitted models\", alpha=0.8)\n",
    "ax[0].plot(tboost_mean.values[::step], color = 'red', label = \"transfer boosted models\",alpha=0.8)\n",
    "\n",
    "ax[0].plot(real_probas_sorted.values[::step], color = 'black', label = \"ideal model\", linewidth=3)\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist(model_std.values, color = 'blue', label = \"refitted models\", histtype='step', linewidth=3)\n",
    "ax[1].hist(tboost_std.values, color = 'red', label = \"transfer boosted models\",  histtype='step',linewidth=3)\n",
    "ax[1].set_title(\"std dev distribution\")\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "ax.hist(model_std.values, color = 'blue', label = \"refitted models\", histtype='step')\n",
    "ax.hist(tboost_std.values, color = 'red', label = \"transfer boosted models\",  histtype='step')\n",
    "ax.set_title(\"std dev distribution\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "handed-kuwait",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(15,8))\n",
    "ax[0].plot(model_std.values[::50], color = 'blue', label = \"refitted models\")\n",
    "ax[0].plot(tboost_std.values[::50], color = 'red', label = \"transfer boosted models\")\n",
    "ax[0].set_title(\"Sorted std dev\")\n",
    "ax[1].hist(model_std.values, color = 'blue', label = \"refitted models\", histtype='step')\n",
    "ax[1].hist(tboost_std.values, color = 'red', label = \"transfer boosted models\",  histtype='step')\n",
    "ax[1].set_title(\"std dev distribution\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-sarah",
   "metadata": {},
   "source": [
    "# Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_roc_auc = roc_auc_score(y2_valid, real_probas)\n",
    "ideal_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_tboost_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_tboost = probas_tboost_df.apply(lambda x: roc_auc_score(y2_valid,x))\n",
    "aucs_model = probas_model_df.apply(lambda x: roc_auc_score(y2_valid,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.hist(aucs_model.values, color = 'blue', label = \"refitted models\", histtype='step', linewidth=3)\n",
    "ax.hist(aucs_tboost.values, color = 'red', label = \"transfer boosted models\",  histtype='step', linewidth=3)\n",
    "ax.set_title(\"AUCs distribution\")\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_tboost.mean(), aucs_tboost.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_model.mean(), aucs_model.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-cabin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
